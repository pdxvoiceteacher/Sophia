#!/usr/bin/env python3
from __future__ import annotations
import argparse, hashlib, json, platform, subprocess, sys, statistics, re, math
from datetime import datetime, timezone
from pathlib import Path
import sys
import os

# --- TEL step events (module-level) ---
def _utc_now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()

def _tel_events_path(outdir: Path) -> Path:
    return outdir / "tel_events.jsonl"

def _tel_emit(outdir: Path, obj: dict) -> None:
    p = _tel_events_path(outdir)
    p.parent.mkdir(parents=True, exist_ok=True)
    with p.open("a", encoding="utf-8", newline="\n") as f:
        f.write(json.dumps(obj, ensure_ascii=False, sort_keys=True) + "\n")

def _step_start(outdir: Path, name: str) -> float:
    t0 = datetime.now(timezone.utc).timestamp()
    _tel_emit(outdir, {"event":"step_start","name":name,"ts":_utc_now_iso(),"run_id":outdir.name})
    return t0

def _step_end(outdir: Path, name: str, t0: float, status: str = "ok", details: dict | None = None) -> None:
    dt_ms = int((datetime.now(timezone.utc).timestamp() - t0) * 1000)
    obj = {"event":"step_end","name":name,"ts":_utc_now_iso(),"run_id":outdir.name,"status":status,"duration_ms":dt_ms}
    if details:
        obj["details"] = details
    _tel_emit(outdir, obj)

def _step_error(outdir: Path, name: str, t0: float, err: Exception) -> None:
    dt_ms = int((datetime.now(timezone.utc).timestamp() - t0) * 1000)
    _tel_emit(outdir, {"event":"step_error","name":name,"ts":_utc_now_iso(),"run_id":outdir.name,"duration_ms":dt_ms,"error":repr(err)})
# --- /TEL step events ---



### UCC OUT PARSE (ALWAYS)
# Always detect an output directory and always wire UCC_TEL_EVENTS_OUT.
# Supports both --out and --outdir forms to avoid guided/unguided mismatch.
_OUT_ALWAYS = None
for i, a in enumerate(sys.argv):
    if a in ("--out", "--outdir") and i + 1 < len(sys.argv):
        _OUT_ALWAYS = sys.argv[i + 1]
        break
    if a.startswith("--out="):
        _OUT_ALWAYS = a.split("=", 1)[1]
        break
    if a.startswith("--outdir="):
        _OUT_ALWAYS = a.split("=", 1)[1]
        break

if _OUT_ALWAYS:
    from pathlib import Path as _Path
    os.environ["UCC_TEL_EVENTS_OUT"] = str(_Path(_OUT_ALWAYS) / "ucc_tel_events.jsonl")
    _Path(_OUT_ALWAYS).mkdir(parents=True, exist_ok=True)
    (_Path(_OUT_ALWAYS) / "ucc_tel_events.jsonl").write_text("", encoding="utf-8", newline="\n")
### /UCC OUT PARSE (ALWAYS)


# --- TEL events flag pre-parse (parser-agnostic) ---
_TEL_EVENTS_EMIT = False
_out = None
if "--emit-tel-events" in sys.argv:
    _TEL_EVENTS_EMIT = True
    sys.argv.remove("--emit-tel-events")
    # Auto-wire UCC step-event sink to <out>/ucc_tel_events.jsonl
    for i, a in enumerate(sys.argv):
        if a == "--out" and i + 1 < len(sys.argv):
            _out = sys.argv[i + 1]
            break
        if a.startswith("--out="):
            _out = a.split("=", 1)[1]
            break

if _out and not os.environ.get("UCC_TEL_EVENTS_OUT"):
    from pathlib import Path as _Path
    os.environ["UCC_TEL_EVENTS_OUT"] = str(_Path(_out) / "ucc_tel_events.jsonl")
    # Ensure TEL events file exists whenever --emit-tel-events is requested (even if later empty)
    _Path(_out).mkdir(parents=True, exist_ok=True)
    (_Path(_out) / "tel_events.jsonl").write_text("", encoding="utf-8", newline="\n")
    (_Path(_out) / "ucc_tel_events.jsonl").write_text("", encoding="utf-8", newline="\n")
# --- /TEL events flag pre-parse ---


def _safe_relpath(p: Path, base: Path) -> str:
    try:
        return str(p.relative_to(base)).replace('\\', '/')
    except Exception:
        return str(p.resolve()).replace('\\', '/')


# --- TEL flag pre-parse (parser-agnostic) ---
_TEL_EMIT = False
if "--emit-tel" in sys.argv:
    _TEL_EMIT = True
    sys.argv.remove("--emit-tel")

# --- /TEL flag pre-parse ---


REPO = Path(__file__).resolve().parents[2]

def sha256_file(p: Path) -> str:
    h = hashlib.sha256()
    with p.open("rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()

def run(cmd: list[str], cwd: Path | None = None) -> None:
    subprocess.run(cmd, cwd=str(cwd) if cwd else None, check=True)

def load_json_bom_safe(p: Path):
    return json.loads(p.read_text(encoding="utf-8-sig"))

def git_commit() -> str:
    try:
        return subprocess.check_output(["git", "rev-parse", "HEAD"], text=True).strip()
    except Exception:
        return "unknown"

def clamp01(x: float) -> float:
    return max(0.0, min(1.0, float(x)))

def perf_E_from_konomi(smoke_out: Path) -> tuple[float, float, int]:
    """
    Compute E proxy from numeric substrings in KONOMI result CSVs.
    Returns (E, perf_index, count_values).
    """
    csvs = [
        smoke_out / "evgpu" / "evgpu_matmul_results.csv",
        smoke_out / "femto" / "femto_async_results.csv",
        smoke_out / "blockarray" / "blockarray_results.csv",
        smoke_out / "cube" / "cube_results.csv",
    ]

    num_re = re.compile(r"[-+]?\d*\.?\d+(?:[eE][-+]?\d+)?")
    vals: list[float] = []

    for fp in csvs:
        if not fp.exists():
            continue
        try:
            txt = fp.read_text(encoding="utf-8-sig", errors="ignore")
            for s in num_re.findall(txt):
                try:
                    v = float(s)
                except Exception:
                    continue
                if math.isfinite(v):
                    vals.append(v)
        except Exception:
            continue

    if not vals:
        return (1.0, 0.0, 0)

    filtered = []
    for v in vals[:10000]:
        av = abs(v)
        # drop huge integer-like IDs
        if av > 1000.0 and float(int(av)) == av:
            continue
        filtered.append(av)

    if not filtered:
        filtered = [abs(v) for v in vals[:10000]]

    logs = [math.log1p(x) for x in filtered[:4000]]
    perf_index = float(sum(logs) / max(1, len(logs)))

    # IMPORTANT: scale is chosen to keep E high enough for Psi gates.
    # If perf_index is ~5-10, dividing by 80 keeps E ~0.88-0.94.
    E = math.exp(-perf_index / 80.0)
    E = clamp01(E)
    return (E, perf_index, len(filtered))

def run_smoke(smoke_out: Path, quick: bool) -> Path:
    smoke_out.mkdir(parents=True, exist_ok=True)
    run([
        sys.executable,
        "python/experiments/konomi_smoke/run_smoke.py",
        "--quick" if quick else "--full",
        "--outdir", str(smoke_out)
    ], cwd=REPO)
    summary = smoke_out / "konomi_smoke_summary.json"
    if not summary.exists():
        raise SystemExit(f"Missing expected smoke summary: {summary}")
    return summary

def run_ucc_coverage(input_path: Path, outdir: Path) -> Path:
    """
    Runs konomi_smoke_coverage.yml and returns the path to audit_bundle.json.
    """
    outdir.mkdir(parents=True, exist_ok=True)
    
    
    run([sys.executable, "-m", "ucc.cli", "run",
         "ucc/modules/konomi_smoke_coverage.yml",
         "--input", str(input_path),
         "--outdir", str(outdir)], cwd=REPO)
    audit_bundle = outdir / "audit_bundle.json"
    if not audit_bundle.exists():
        raise SystemExit(f"Missing audit_bundle.json at {audit_bundle}")
    return audit_bundle

def T_Es_from_audit_bundle(audit_bundle: Path) -> tuple[float, float]:
    """
    Derive T and Es from UCC coverage audit bundle.
    - T from section_coverage (or present_count/required_count)
    - Es from flags.sections_complete (1/0)
    """
    b = load_json_bom_safe(audit_bundle)
    m = b.get("metrics", {}) or {}
    fl = b.get("flags", {}) or {}

    if "section_coverage" in m:
        T = float(m.get("section_coverage", 1.0))
    elif ("present_count" in m) and ("required_count" in m):
        req = float(m.get("required_count", 1.0))
        pres = float(m.get("present_count", req))
        T = pres / req if req > 0 else 1.0
    else:
        T = 1.0

    T = clamp01(T)
    Es = 1.0 if bool(fl.get("sections_complete", fl.get("overall_pass", True))) else 0.0
    Es = clamp01(Es)
    return (T, Es)

def metric_vec(E: float, T: float, Es: float) -> dict[str, float]:
    Psi = clamp01(float(E) * float(T))
    return {"E": clamp01(E), "T": clamp01(T), "Psi": Psi, "Es": clamp01(Es)}

def drift(a: dict[str, float], b: dict[str, float]) -> float:
    keys = ["E", "T", "Psi", "Es"]
    return sum(abs(a[k] - b[k]) for k in keys) / len(keys)



def _utc_now_z() -> str:
    return datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace("+00:00", "Z")


def _load_network_profile() -> str:
    policy_path = REPO / "config" / "network_policy_v1.json"
    if not policy_path.exists():
        return "witness_only"
    try:
        policy = load_json_bom_safe(policy_path)
    except Exception:
        return "witness_only"
    profile = str(policy.get("profile") or "witness_only")
    if profile not in {"witness_only", "reproducible_audit", "full_relay"}:
        return "witness_only"
    return profile


def _write_evidence_and_consensus(outdir: Path, artifacts: list[dict[str, str]]) -> None:
    profile = _load_network_profile()
    created_at = _utc_now_z()
    run_id = outdir.name
    artifact_entries = []
    for art in artifacts:
        rel = art["path"]
        p = (outdir / rel)
        size = p.stat().st_size if p.exists() else 0
        artifact_entries.append(
            {
                "path": rel,
                "content_type": "application/json",
                "sha256": art["sha256"],
                "size_bytes": size,
                "classification": "evidence",
                "encrypted": False,
                "transport": {
                    "transport_sha256": art["sha256"],
                    "transport_size_bytes": size,
                },
            }
        )

    evidence = {
        "schema": "evidence_bundle_v1",
        "bundle_id": f"bundle-{run_id}",
        "created_at_utc": created_at,
        "producer": {
            "node_id": "node_local_runner",
            "software": "sophia",
            "version": "v0.1",
        },
        "policy": {
            "network_profile": profile,
            "share_scopes": ["hashes/*", "metrics/*", "attestations/*"],
            "peer_set": [],
        },
        "artifacts": artifact_entries,
    }
    try:
        from tools.security.swarm_crypto import compute_evidence_bundle_hash
        evidence["bundle_sha256"] = compute_evidence_bundle_hash(evidence)
    except Exception:
        evidence["bundle_sha256"] = hashlib.sha256(
            json.dumps(evidence, ensure_ascii=False, sort_keys=True, separators=(",", ":")).encode("utf-8")
        ).hexdigest()
    (outdir / "evidence_bundle.json").write_text(json.dumps(evidence, indent=2, sort_keys=True), encoding="utf-8")

    peer_path = outdir / "peer_attestations.json"
    peer_present = False
    peer_items = []
    if peer_path.exists():
        try:
            parsed = load_json_bom_safe(peer_path)
            peer_items = parsed if isinstance(parsed, list) else (parsed.get("attestations") or [])
            peer_present = bool(peer_items)
        except Exception:
            peer_present = False

    central_path = outdir / "attestations.json"
    central_present = central_path.exists()
    consensus = "insufficient"
    if peer_present:
        peer_pass = 0
        peer_fail = 0
        for item in peer_items:
            payload = item.get("payload") if isinstance(item, dict) else {}
            results = payload.get("results") if isinstance(payload, dict) else {}
            status = str((results or {}).get("status", "pending")).lower()
            if status == "pass":
                peer_pass += 1
            elif status == "fail":
                peer_fail += 1
        if peer_pass > 0 and peer_fail == 0:
            consensus = "convergent"
        elif peer_fail > 0:
            consensus = "divergent"

    consensus_doc = {
        "schema": "consensus_summary_v1",
        "bundle_hash": evidence["bundle_sha256"],
        "computed_at_utc": created_at,
        "inputs": {
            "central_attestations_present": central_present,
            "peer_attestations_present": peer_present,
        },
        "central": {
            "total": 1 if central_present else 0,
            "pass": 0,
            "fail": 0,
            "pending": 1 if central_present else 0,
        },
        "peers": {
            "total": len(peer_items),
            "pass": 0,
            "fail": 0,
            "pending": len(peer_items),
            "weighted_pass": 0.0,
            "weighted_fail": 0.0,
            "weighted_pending": float(len(peer_items)),
        },
        "consensus": consensus,
        "policy_gate": {
            "network_profile": profile,
            "required_for": ["publish", "export"],
            "satisfied": bool(consensus == "convergent"),
        },
    }
    if peer_items:
        peer_pass = 0
        peer_fail = 0
        for item in peer_items:
            payload = item.get("payload") if isinstance(item, dict) else {}
            results = payload.get("results") if isinstance(payload, dict) else {}
            status = str((results or {}).get("status", "pending")).lower()
            if status == "pass":
                peer_pass += 1
            elif status == "fail":
                peer_fail += 1
        peer_pending = len(peer_items) - peer_pass - peer_fail
        consensus_doc["peers"].update(
            {
                "pass": peer_pass,
                "fail": peer_fail,
                "pending": peer_pending,
                "weighted_pass": float(peer_pass),
                "weighted_fail": float(peer_fail),
                "weighted_pending": float(peer_pending),
            }
        )

    (outdir / "consensus_summary.json").write_text(json.dumps(consensus_doc, indent=2, sort_keys=True), encoding="utf-8")

def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--out", required=True)
    ap.add_argument("--quick", action="store_true")
    ap.add_argument("--perturbations", type=int, default=3)
    args = ap.parse_args()

    outdir = Path(args.out).resolve()
    outdir.mkdir(parents=True, exist_ok=True)

    # Base run
    base_smoke = outdir / "konomi_smoke_base"
    t0_base = _step_start(outdir, "konomi_smoke_base")
    base_summary = run_smoke(base_smoke, args.quick)

    base_cov = outdir / "ucc_cov_out"
    _step_end(outdir, "konomi_smoke_base", t0_base, "ok", details={"out":"konomi_smoke_base"})
    t0_ucc_base = _step_start(outdir, "ucc_cov_base")
    base_bundle = run_ucc_coverage(base_summary, base_cov)
    _step_end(outdir, "ucc_cov_base", t0_ucc_base, "ok", details={"out":"ucc_cov_out"})

    E_base, perf_index_base, nvals_base = perf_E_from_konomi(base_smoke)
    T_base, Es_base = T_Es_from_audit_bundle(base_bundle)
    base_vec = metric_vec(E_base, T_base, Es_base)

    distances: list[float] = []
    pert_results = []

    for i in range(1, max(0, args.perturbations) + 1):
        smoke_i = outdir / f"konomi_smoke_perturb_{i}"
        t0_p = _step_start(outdir, f"konomi_smoke_perturb_{i}")
        summary_i = run_smoke(smoke_i, args.quick)

        cov_i = outdir / f"ucc_cov_perturb_{i}"
        bundle_i = run_ucc_coverage(summary_i, cov_i)
        _step_end(outdir, f"konomi_smoke_perturb_{i}", t0_p, "ok", details={"i": i, "out": f"konomi_smoke_perturb_{i}"})

        E_i, perf_index_i, nvals_i = perf_E_from_konomi(smoke_i)
        T_i, Es_i = T_Es_from_audit_bundle(bundle_i)
        vec_i = metric_vec(E_i, T_i, Es_i)

        d_i = drift(base_vec, vec_i)
        distances.append(d_i)

        pert_results.append({
            "i": i,
            "audit_bundle_path": _safe_relpath(bundle_i, outdir),
            "metrics": vec_i,
            "drift_from_base": d_i,
            "perf_index": perf_index_i,
            "perf_values_count": nvals_i,
            "T_source": "audit_bundle.section_coverage",
            "Es_source": "audit_bundle.flags.sections_complete"
        })

    deltaS = float(statistics.mean(distances)) if distances else 0.0
    lam = float(max(distances)) if distances else 0.0

    pert_path = outdir / "perturbations.json"
    pert_doc = {
        "kind": "telemetry_perturbations.v1",
        "base": {
            "audit_bundle_path": _safe_relpath(base_bundle, outdir),
            "metrics": base_vec,
            "perf_index": perf_index_base,
            "perf_values_count": nvals_base
        },
        "perturbations": pert_results,
        "deltaS": deltaS,
        "lambda": lam
    }
    pert_path.write_text(json.dumps(pert_doc, indent=2, sort_keys=True), encoding="utf-8")

    # Final telemetry.json
    metrics = dict(base_vec)
    metrics["DeltaS"] = deltaS
    metrics["Lambda"] = lam

    artifacts = [
        {"path": _safe_relpath(base_summary, outdir), "sha256": sha256_file(base_summary)},
        {"path": _safe_relpath(base_bundle, outdir), "sha256": sha256_file(base_bundle)},
        {"path": _safe_relpath(pert_path, outdir), "sha256": sha256_file(pert_path)}
    ]

    connector_override = {
        "type": os.environ.get("SOPHIA_CONNECTOR_TYPE"),
        "endpoint": os.environ.get("SOPHIA_CONNECTOR_ENDPOINT"),
        "model": os.environ.get("SOPHIA_CONNECTOR_MODEL"),
    }

    telemetry = {
        "schema_id": "coherencelattice.telemetry_run.v1",
        "version": 1,
        "run_id": outdir.name,
        "created_at": datetime.now(timezone.utc).isoformat(),
        "environment": {
            "python": sys.version.replace("\\n", " "),
            "platform": platform.platform(),
            "git_commit": git_commit(),
            "connector_override": connector_override
        },
        "metrics": metrics,
        "flags": {
            "telemetry_ok": True,
            "perturbations_count": len(distances),
            "deltaS_from_perturbations": deltaS,
            "lambda_from_perturbations": lam,
            "perf_index_base": perf_index_base,
            "perf_values_count_base": nvals_base
        },
        "artifacts": artifacts,
        "ucc": {
            "audit_bundle_path": _safe_relpath(base_bundle, outdir),
            "audit_bundle_sha256": sha256_file(base_bundle)
        },
        "notes": "Telemetry derived without telemetry_snapshot module. E from KONOMI CSV numeric substrings; T/Es from UCC coverage audit_bundle; Psi=E*T; ÃŽâ€S/ÃŽâ€º from perturbation drift."
    }

    t0_write = _step_start(outdir, "write_telemetry_json")
    out_json = outdir / "telemetry.json"
    out_json.write_text(json.dumps(telemetry, indent=2, sort_keys=True), encoding="utf-8")
    _write_evidence_and_consensus(outdir, artifacts)
    print(f"[run_wrapper] wrote {out_json}")
    _step_end(outdir, "write_telemetry_json", t0_write, "ok", details={"path":"telemetry.json"})
    return 0

if __name__ == "__main__":
    # Preserve original entrypoint semantics, but allow TEL emission after run completion.
    try:
        _rc = main()
    except SystemExit as _se:
        _rc = _se.code

    if _rc is None:
        _rc = 0

    # --- TEL/UCC post-run emission ---
    # We separate TEL emission from UCC emission to avoid schema mixing.
    # TEL emission is optional. UCC event stream is always written when --out is present.

    def _find_out(argv):
        for i, a in enumerate(argv):
            if a in ("--out", "--outdir") and i + 1 < len(argv):
                return argv[i + 1]
            if a.startswith("--out="):
                return a.split("=", 1)[1]
            if a.startswith("--outdir="):
                return a.split("=", 1)[1]
        return None

    _out = _find_out(sys.argv)
    if not _out:
        raise SystemExit(_rc)

    _out_dir = Path(_out)
    _telemetry_path = _out_dir / "telemetry.json"
    if not _telemetry_path.exists():
        raise SystemExit(_rc)

    # Load telemetry
    _telemetry_data = json.loads(_telemetry_path.read_text(encoding="utf-8"))

    # Always ensure UCC stream exists and contains at least one metrics-bearing event.
    # This preserves auditability even if internal UCC modules do not emit events.
    try:
        ucc_path = _out_dir / "ucc_tel_events.jsonl"
        ucc_path.parent.mkdir(parents=True, exist_ok=True)
        if not ucc_path.exists():
            ucc_path.write_text("", encoding="utf-8", newline="\n")

        m = (_telemetry_data.get("metrics", {}) if isinstance(_telemetry_data, dict) else {}) or {}
        lam = m.get("Λ", m.get("lambda", m.get("Lambda", m.get("Lambda_T"))))

        meta = {}
        if lam is not None:
            lam_f = float(lam)
            meta = {
                "lambda": lam_f,
                "lambda_warn": 0.80,
                "lambda_gate": "review" if lam_f >= 0.80 else "proceed",
            }

        evt = {"seq": 1, "kind": "ucc_lambda_metrics", "data": {"metrics": m, "meta": meta}}
        with ucc_path.open("a", encoding="utf-8", newline="\n") as f:
            f.write(json.dumps(evt, ensure_ascii=False, sort_keys=True, separators=(",", ":")) + "\n")

        print(f"[ucc_tel_events] wrote: {ucc_path}")
    except Exception as e:
        print(f"[ucc_tel_events] failed: {e}", file=sys.stderr)

    # TEL emission (optional)
    if globals().get("_TEL_EMIT", False) or globals().get("_TEL_EVENTS_EMIT", False):
        try:
            from konomi.tel.build import build_tel_and_events_for_out_dir
            from konomi.tel.events import write_events_jsonl

            _tel, _events = build_tel_and_events_for_out_dir(_out_dir, _telemetry_data, max_depth=2)

            if globals().get("_TEL_EMIT", False):
                _tel.write_json(_out_dir / "tel.json")
                _telemetry_data["tel_summary"] = _tel.summary()
                (_out_dir / "telemetry.json").write_text(
                    json.dumps(_telemetry_data, ensure_ascii=False, sort_keys=True, indent=2) + "\n",
                    encoding="utf-8",
                    newline="\n",
                )
                print("[tel] updated telemetry.json with tel_summary")
                print(f"[tel] wrote: {_out_dir / 'tel.json'}")

            if globals().get("_TEL_EVENTS_EMIT", False):
                (_out_dir / "tel_events.jsonl").write_text("", encoding="utf-8", newline="\n")
                write_events_jsonl(_events, _out_dir / "tel_events.jsonl")
                print(f"[tel_events] wrote: {_out_dir / 'tel_events.jsonl'}")

        except Exception as _e:
            print(f"[tel] failed: {_e}", file=sys.stderr)
            raise

    # --- /TEL/UCC post-run emission ---
    raise SystemExit(_rc)
